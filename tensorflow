#!/usr/bin/env python
#promote some action
'''
Created on 18-7-30
@author: minghui_J
use tensorflow to train weibo classify category label
'''
import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import tensorflow as tf
import numpy as np
#from tensorflow.examples.tutorials.mnist import input_data
tf.set_random_seed(1)

#mnist = input_data.read_data_sets("MNIST_data",one_hot=True)
DataR_root = '/home/jmh/DataProcess/New1.txt'
f = open(DataR_root,'r')
#lines = f.readlines()

#lr = 0.001
lr = 0.8
lr_decay = 0.99
training_iters = 15000
batch_size = 64
n_inputs = 3
n_steps = 1
n_hidden_units = 128
n_classes = 8

x = tf.placeholder(tf.float32,[None,n_steps,n_inputs])
y = tf.placeholder(tf.float32,[None,n_classes])

weights={'in':tf.Variable(tf.random_normal([n_inputs,n_hidden_units],stddev=1,seed=1)),'out':tf.Variable(tf.random_normal([n_hidden_units,n_classes],stddev=1,seed=1))}
biases={'in':tf.Variable(tf.constant(0.1,shape=[n_hidden_units,])),'out':tf.Variable(tf.constant(0.1,shape=[n_classes,]))}

def RNN(X,weights,biases):
	X = tf.reshape(X,[-1,n_inputs])
	
	X_in = tf.nn.relu(tf.matmul(X,weights['in'])+biases['in'])
	X_in = tf.reshape(X_in,[-1,n_steps,n_hidden_units])
	lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units,forget_bias=1.0,state_is_tuple=True)
	init_state = lstm_cell.zero_state(batch_size,dtype=tf.float32)
	outputs,final_state = tf.nn.dynamic_rnn(lstm_cell,X_in,initial_state=init_state,time_major=False)
	#results = tf.matmul(final_state[1],weights['out'])+biases['out']
	outputs = tf.unstack(tf.transpose(outputs,[1,0,2]))
	results = tf.nn.relu(tf.matmul(outputs[-1],weights['out'])+biases['out'])
	return results
global_step = tf.Variable(0,trainable=False)
pred = RNN(x,weights,biases)
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))
lr = tf.train.exponential_decay(lr,global_step,training_iters,lr_decay)
train_op = tf.train.AdamOptimizer(lr).minimize(cost)
correct_pred = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))
accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))
init = tf.global_variables_initializer()
with tf.Session() as sess:
	sess.run(init)
	step = 0
	while step *batch_size< training_iters:
		#batch_xs,batch_ys = mnist.train.next_batch(batch_size)
		batch_xs = []
		batch_ys = []
		i = 1
		#for line in lines:
		while(1):
			line = f.readline()
			if(not line):
				break
			line = line.split(' ')
			batch_ys1 = [0,0,0,0,0,0,0,0]
			batch_xs1 = [float(line[0]),float(line[1]),float(line[2])]
			batch_xs.append(batch_xs1)
			batch_ys1[int(line[3])] = 1 
			batch_ys.append(batch_ys1)
			if(i%batch_size==0):
				batch_xs = np.array(batch_xs,dtype=np.float)
				batch_ys = np.array(batch_ys,dtype=np.float)
				break
			i += 1
		batch_xs = batch_xs.reshape([batch_size,n_steps,n_inputs])
		sess.run([train_op],feed_dict={x:batch_xs,y:batch_ys,})
		if step % 5 ==0:
			print sess.run(accuracy,feed_dict={x:batch_xs,y:batch_ys})
		step += 1
f.close()
